{
  "topic": "test logging",
  "iteration": 0,
  "sources": [
    {
      "url": "https://testsigma.com/blog/test-log/",
      "title": "What is a Test log? A Complete Guide - Testsigma",
      "content": "A Test Log is nothing but a document to keep track of all the tests that are executed. It logs test cases, results, and any issues arising during testing. It helps a tester to stay on top of their work and provides a thorough report of the testing process.\nIn this blog post, we\u2019ll dive deep into test logs and show you how to use them like a pro.\nTable Of Contents\nWhat is a Test Log?\nA test log is a detailed record documenting the activities carried out during software development or system testing. It is an essential tool for tracking and managing the testing process. The log contains information such as the names and descriptions of test cases, dates of execution, results, and any defects encountered. It summarizes the testing progress, enabling testers to identify patterns or trends in failures and successes. Moreover, a test log helps troubleshoot by providing valuable insights into the steps taken during testing.\nComponents of a Test Log\nThe generation of test logs following each test must consist of entries that precisely record various aspects of the test and furnish comprehensive and relevant information. A test log comprises multiple components, namely:\n1. Date and Time: The log must encompass the date and time of the event, failure, or any other significant issue.\n2. A concise description of the event being logged must be furnished.\n3. Status: The log must indicate the status of distinct events observed by the team.\n4. Contextual Details: Additional details and pertinent information must be provided as and when required.\n5. Anomalies and Errors: Details concerning any anomalies and errors identified during testing must be documented in the log.\nTest Log Template\nGenerating a test log report is an important aspect of efficient test management and documentation. It facilitates monitoring testing activities, detecting anomalies, and presenting a comprehensive view of the testing procedure to the relevant parties. The following are essential elements to take into account when developing a test log report:\n1. Test Case Information: Include details about each test case executed, such as the unique identifier, description, preconditions, and expected results. This information helps in understanding the purpose and scope of each test case.\n2. Test Execution Details: Record the date and time each test case was executed. This allows for easy tracking of when tests were performed and helps identify any patterns or trends in the testing process.\n3. Test Environment: Document the specific environment in which the tests were conducted. This includes hardware configurations, software versions, operating systems, and browsers This information is essential for reproducing issues and ensuring consistency across testing environments.\n4. Test Results: It is important to record any deviations or anomalies observed during the testing process to provide a comprehensive overview of the test results.\nAdvantages\nLet us discuss some of the advantages of using a Test Log:\n- Provides a record of all tests conducted, allowing for easy reference and progress tracking.\n- Helps identify patterns or trends in test results, enabling the identification of recurring issues or areas for improvement.\n- Facilitates collaboration among team members by providing a centralized location for sharing test information and findings.\n- Allows for better organization and categorization of tests, making locating specific tests or groups of tests easier.\n- Enables traceability by documenting the steps taken during testing, making reproducing and debugging issues easier.\n- Enhances stakeholder communication by providing a clear overview of the testing process and outcomes.\n- Supports risk management by highlighting potential areas of concern or high-risk tests that require additional attention.\n- Assists in compliance with regulatory standards or industry best practices by ensuring that all necessary tests have been conducted and documented.\nCreating a Test Log: Step-by-step Guide\nStep 1: Define the Purpose and Scope\n\u2013 Identify the purpose of creating a test log, such as tracking test progress, documenting test results, or identifying defects.\n\u2013 Determine the scope of the test log, including which tests or test cycles it will cover.\nStep 2: Identify Required Information\n\u2013 Determine what information needs to be included in the test log. This may include details like test case ID, description, expected result, actual result, status (pass/fail), date/time executed, tester name, and other relevant fields.\nStep 3: Choose a Format\n\u2013 Decide on the format for your test log. It can be a spreadsheet (e.g., Excel), a document (e.g., Word), or even a specialized testing tool with built-in logging capabilities.\nStep 4: Create Columns and Headers\n\u2013 Set up columns in your chosen format to accommodate all the required information identified in Step 2.\nFor example, if you are using Excel, you can create columns such as \u201cTest Case ID,\u201d \u201cDescription,\u201d \u201cExpected Result,\u201d and \u201cActual Result\u201d to organize and track the necessary information.\nImportance of Test Log in Test Automation\nHere are some major reasons why the test log is crucial in test automation:\n1. Provides detailed information about the test execution process, including the test case status, test environment, and test data.\n2. Enables testers to trace back to the source of an issue, facilitating faster debugging and fixing.\n3. Helps testers generate accurate and comprehensive reports, improving stakeholder communication.\n4. Allows for better collaboration among team members, as everyone can access the same information.\n5. Enhances the reliability and consistency of the testing process, leading to better-quality software products. In conclusion, the test log is a critical component of test automation tools that facilitates efficient and effective testing.\nTest Log Example\nHere we are considering a test log generated by an automated test case in test automation tool Testsigma.\nStep-1\nTo begin, access the Testsigma platform and register a new account. Once successfully registered, create a new project and a New Test case. Proceed to write Test steps for execution in the test case. Below is an image depicting the test steps formulated for verifying invalid credentials on a travel site login page.\nStep-2\nUpon successful execution of the test cases, the test results reveal a state of \u201cPassed.\u201d One must navigate to the Logs bar by clicking on it to access the test logs. Referencing the given image below, the details can be observed. Clicking on the Logs icon provides access to the Test logs.\nStep-3:\nIn the final stage, a log list is presented to provide a detailed overview of all the processes that have occurred in the backend. Testsigma facilitates recording step-by-step processes, enabling the display of a complete list.\nSummary\nMaintaining a Test log is essential for documenting and monitoring testing procedures. Testsigma, an automation platform, simplifies this process. Testsigma operates at 5x speed and is a fully-managed, Cloud-based test automation platform.\nAutomate your tests for web, mobile, desktops, and APIs, 10x faster with Testsigma, with easy-to-traverse test logs\nFrequently Asked Questions\nWhat Are the Different Types of Logs in Testing?\nThe different types of logs in testing include system logs, application logs, and test execution logs. System logs capture information about the operating system and hardware, application logs record events within the software, and test execution logs document the details of test runs, such as pass/fail status and error messages.\nWhat is Log-in Automation Testing?\nLog-in automation testing refers to automating the testing of log-in functionality in software applications. It involves creating scripts or using tools to simulate user log-ins and validate their success. This type of testing helps ensure that log-in processes work correctly and efficiently.",
      "domain": "testsigma.com",
      "retrieved_at": "2025-12-15 07:35:37.656280",
      "query_used": "test logging"
    },
    {
      "url": "https://zencoder.ai/glossary/test-log",
      "title": "Test Log: Definition, Purpose & Benefits",
      "content": "What is a Test Log?\nA test log is a record that details the activities and outcomes of software testing. It tracks the execution of test cases and provides information about any issues encountered during the testing process.\nPurpose of Test Log\nThe main purpose of a test log is to document the testing process, including what tests were run, their results, and any problems that arose. This helps in understanding the testing coverage and tracking the progress of bug fixes or enhancements.\nHow Does it Work?\n- Recording: Captures details of each test case executed, including start and end times, and results.\n- Tracking: Logs any defects or issues found during testing.\n- Reviewing: Allows review of the test results to analyze performance and identify trends.\n- Reporting: Generates reports from the recorded data for further analysis and decision-making.\nBenefits\n- Transparency: Provides a clear view of what has been tested and the outcomes.\n- Accountability: Helps in tracking who performed each test and the results.\n- Analysis: Facilitates the analysis of test results to identify patterns and areas for improvement.\n- Documentation: Serves as a record for future reference, aiding in audits and reviews.\nConclusion\nA test log is an essential tool in software testing, offering a detailed record of test execution and outcomes. It enhances transparency, accountability, and analysis, contributing to better management and improvement of the testing process.",
      "domain": "zencoder.ai",
      "retrieved_at": "2025-12-15 07:35:39.727168",
      "query_used": "test logging"
    },
    {
      "url": "https://www.qodo.ai/glossary/test-log/",
      "title": "What is Test Log - Qodo",
      "content": "Test Log\nWhat is a Test Log?\nIn software testing, a Test Log meticulously records the activities and outcomes of test cases throughout a designated phase. This chronological account-detailing which tests were executed, their conductors, performance dates, and each test\u2019s result-is critical to the overall testing process. Without it, we couldn\u2019t gauge our progress or identify potential issues. The Test Log also functions as a map test session, guiding the testing process through documented evidence and observations. The importance of logs in testing cannot be understated, as they provide a comprehensive record of the testing process.\nA Test Log exhibits key characteristics, such as:\n- Chronological record: Herein lies a sequential record of all testing activities. The record presents an event timeline (a chronological register that encapsulates the occurrences throughout the testing process).\n- Details of the test: Each test case\u2019s specific details \u2013 the identifier, description, execution date, and tester name; along with any pertinent environmental or setup information.\n- Observations and results: Each test case\u2019s outcomes \u2013 whether they yield a pass, fail, or remain incomplete \u2013 are documented in the Test Log.\n- Issue tracking: The Test Log often references related issue reports or defect IDs for further investigation and resolution when a test fails or we identify a problem.\n- Serving as an audit trail, a Test Log aids in tracking the testing effort and progress over time. A critical function for project management, quality assurance, and compliance purposes.\n- The analysis and reporting tool: Test Logs are used to analyze the testing process, identify patterns or recurrent issues, and generate reports on two key aspects: the software\u2019s quality and the overall effectiveness of our testing efforts.\nThe Test Log, in essence, ensures transparency and accountability within the software testing process.\n[blog-subscribe]\nTest Case vs Test Log\nIn the realm of software testing, comprehending the distinction between a Test Log and a Test Case serves unique purposes; furthermore, they contribute distinctively to our testing process.\nTest Case\n- Definition: A specific set of actions, conditions, and inputs serves as a test case to validate the correct functioning of an application\u2019s feature. This detailed document outlines the entire test scenario for a comprehensive evaluation, including prerequisites, test steps, and expected results vs actual results.\n- Goal: The tester\u2019s primary aim with a test case is to navigate through a sequence of steps and validate the specific requirement or functionality in question.\n- Components: A test case consists of the following components: the test case ID and a description that clearly outlines prerequisites and steps for conducting tests. Expected results are also included, often accompanied by an additional element-the requirement or functionality under examination.\n- Usage: During the test execution phase, we employ Test Cases as our foundation for conducting actual testing.\nTest Log\n- Definition: A test log actively records the testing activities, including the execution of test cases by specific individuals. Moreover, this comprehensive document meticulously notes any observed anomalies or defects. Test logs are detailed records documenting testing activities: they pinpoint when test cases were executed, identify their executors, and specify each case\u2019s status (pass/fail). These comprehensive documents also note with precision any observed anomalies or defects.\n- Goal: A historical record of the testing process serves multiple purposes: it tracks and monitors testing progress, provides evidence of test results, and ensures thoroughness in our overall approach to quality assurance.\n- Components: A test log typically includes the date of test execution, the tester\u2019s name, executed test case IDs, the outcomes, and any notes or observations.\n- Usage: Throughout the testing process, we maintain test logs. We utilize these for auditing, tracking progress, and analyzing the outcomes of the testing phase: they serve as crucial indicators in our quality assurance procedures.\nA test case serves the purpose of testing a specific function or feature within the software. On the other hand, a test log is an integral component of the software testing lifecycle. The test log records factual execution details of these test cases, encapsulating results and any pertinent nuances in its comprehensive view of processes involved in the rigorous examination. Both serve distinct functions yet remain crucial for effective quality assurance within any given system.\nImportance of Test Log\nA test log in test automation serves as a crucial tool; it indispensably documents the executions of automated tests, meticulously recording the history of each test run. This form of documentation is not just useful; rather, it is vital because, over time and with its aid, we can identify patterns or anomalies within our automated tests. Actively expediting the pinpointing of failures or irregularities are test logs, thus accelerating debugging and issue resolution. They provide invaluable insights that amplify the efficiency of both automation scripts and overarching testing strategies. Indispensable for audit trails, compliance, and quality assurance, their role in ensuring transparency and accountability in automated testing processes is vital for audit trails, compliance, and quality assurance, making the test log a fundamental part of the log in software testing process.",
      "domain": "qodo.ai",
      "retrieved_at": "2025-12-15 07:35:41.527671",
      "query_used": "test logging"
    },
    {
      "url": "https://www.infoq.com/articles/why-test-logging/",
      "title": "Why and How to Test Logging - InfoQ",
      "content": "Modern log aggregation & search tools provide significant new capabilities for teams building, testing, and running software systems. By treating logging as a core system component, and using techniques such as unique event IDs, transaction tracing, and structured log output, we gain rich insights into application behaviour and health, especially cross-component visibility. This article explains why it is valuable to test aspects of logging and how to do this with modern log aggregation tooling. This approach makes logging a channel or vector to make distributed systems more testable.\nLogging speeds things up overall\nHistorically, many people considered that logging 'slowed down' software. With synchronous file I/O, slow disk storage, and even slower network speeds, this view had some merit, and so we tended to be judicious about what we logged from software running in the live environment. However, with asynchronous file I/O and SSD storage becoming the norm and 1Gb, 10Gb, and even 100Gb Ethernet increasingly common, the performance characteristics of logging now look different.\nThese days, outside time-critical applications such as financial trading and other algorithmically-sensitive situations, pure runtime execution speed is rarely something we should optimize for in software systems. Particularly in the context of distributed systems, cloud, and IoT, we need to consider the time it takes to restore service (often termed 'Mean Time to Recovery', or MTTR) following a failure, as well as the time it takes to determine the cause of problems in upstream (testing) environments.\nModern log aggregation and search tools \u2013 such as ElasticSearch/Logstash/Kibana, LogEntries, Loggly, Sematext, and so on \u2013 provide hugely empowering ways of interacting with our software as it is running through rich user interfaces for exploring application behaviour and programmable REST APIs for searching and correlating events across multiple servers.\nAlthough additional logging may result in a 5%-10% drop in raw runtime execution speed, the detailed information available in a single searchable location helps us to diagnose problems much more quickly, speeding up our incident response and often significantly reducing the time to find that awkward bug!\nThe combination of fast I/O & storage and modern log tooling \u2013 particularly when the tooling is available to all testers and developers \u2013 enables us to treat logging as a key part of our software systems; this leads us to ask: if logging is a key part of our software system, how can we test it?\nParcel tracking as an analogy\nMost of us are familiar with online parcel tracking tools. These allow us to see where our parcel is given a tracking ID. These tools have two interesting features: being able to trace a specific parcel through the delivery network and also showing various different states (or events) that can relate to a parcel at different times.\nWhen tracking a parcel, we can see states such as \u2018Arrived at depot\u2019, In transit\u2019 and \u2018Delivered\u2019; these represent specific states or events and each has an internal identifier (ID) within the system \u2013 an event ID.\nWithin modern asynchronous distributed software systems, we can use a similar technique to trace execution of operations across different component boundaries. To help us do this, we define some of our own event IDs that relate to the system that we\u2019re working with.\nTest for expected events and correlation IDs\nWe should not spend time testing the logging subsystem itself, such as log4net, log4j, etc.; we should assume that the mechanics of logging (writing to disk, rotating log files, flushing buffers, etc.) are already handled. Instead, we should concentrate on ensuring three separate but related things:\n- Events that we expect to occur appear correctly in the log stream\n- Transaction identifiers (aka correlation IDs) flow through the log stream as expected\n- Events are logged at the appropriate level (Info, Error, Debug, etc.) \u2013 if we're using configurable log levels\nOf course, by checking for these things, we exercise the logging subsystem and implicitly test that too. By addressing logging as a testable system component, we also tend to reduce the \u2018time-to-detect\u2019 for problems, increase team engagement, enhance collaboration, and increase software operability.\nWe need to define a set of event type IDs that correspond to useful and interesting actions or execution points in our software. Exactly how many of these IDs you use depends on your software, but at a minimum we have ApplicationStarted and things like DatabaseConnectionFailed or DocumentStoreUnavailable (define additional IDs when these are useful \u2013 don\u2019t try to define all possible events up front).\nFor instance, if we\u2019re building an ecommerce application in C#, we might have:\npublic enum EventID\n{\n// Badly-initialised logging data\nNotSet = 0,\n// An unrecognised event has occurred\nUnexpectedError = 10000,\nApplicationStarted = 20000,\nApplicationShutdownNoticeReceived = 20001,\nPageGenerationStarted = 30000,\nPageGenerationCompleted = 30001,\nMessageQueued = 40000,\nMessagePeeked = 40001,\nBasketItemAdded = 60001,\nBasketItemRemoved = 60002,\nCreditCardDetailsSubmitted = 70001,\n// ...\n}\nWe use human-readable names for the event IDs along with unique integer values that group together related or similar events: here, all event types relating to \u2018Basket\u2019 have integer IDs between 60000 and 69999. When our software reaches this state in the code, it writes the relevant event type ID to the log file along with other log data. This in turn gets picked up by the log aggregation system and made available for searching (via a browser and an API).\nIf we want say to automate a test that expected or unexpected events occurred in the log stream we can query for the event via a simple API call using curl\n.\nFor example, we might want to check that a database query occurred (expected events DatabasePreQuery and DatabasePostQuery) and there was no connection issue (unexpected event DatabaseConnectionFailed).\nThis is the curl command to query the Elasticsearch API (running on localhost) for DatabasePreQuery events (you can find it on Github as well):\n$ curl -XGET 'http://localhost:9200/_search?q=message:DatabasePreQuery&pretty'\nThe result of this query could be, for example (formatted for clarity and added line numbers for reference):\n1 {\n2 \"took\" : 19,\n3 \"timed_out\" : false,\n4 \"_shards\" : {\n5 \"total\" : 20,\n6 \"successful\" : 20,\n7 \"failed\" : 0\n8 },\n9 \"hits\" : {\n10 \"total\" : 1,\n11 \"max_score\" : 11.516103,\n12 \"hits\" : [ {\n13 \"_index\" : \"logstash-2016.05.06\",\n14 \"_type\" : \"logs\",\n15 \"_id\" : \"gHEKyHasRb6GaUhM1gywpg\",\n16 \"_score\" : 11.516103,\n17 \"_source\":{\"message\":\n18 \"[2016-05-06 17:07:42] slim-skeleton.INFO: DatabasePreQuery [] []\",\n19 \"@version\":\"1\",\n20 \"@timestamp\":\"2016-05-06T16:07:42.749Z\",\n21 \"host\":\"vagrant-ubuntu-trusty-64\",\n22 \"path\":\"/var/www/wibble/logs/app.log\"}\n23 } ]\n24 }\n24\n25 }\nLine #10 above shows us that the query matched exactly one log stream entry (total hits was 1), and at line #13 starts the query response, with the actual log message starting at line #18.\nWe can then input these search results into our database query test using our tooling of choice to parse the JSON response and identify which events occurred or not.\nFor example, a basic Ruby implementation of the test (you can find it on Github as well):\n1 require 'json'\n2\n3 file = open(\"Database_prequery_search.json\")\n4 prequery = JSON.parse(file.read)\n5\n6 file = open(\"Database_postquery_search.json\")\n7 postquery = JSON.parse(file.read)\n8\n9 file = open(\"Database_connectionfailed_search.json\")\n10 connectionfailed = JSON.parse(file.read)\n11\n12 expected_prequery_event = (prequery[\"hits\"][\"total\"] == 1)\n13 expected_postquery_event = (postquery[\"hits\"][\"total\"] == 1)\n14 unexpected_connectionfailed_event = (connectionfailed[\"hits\"][\"total\"] == 0)\n15\n16 expected_prequery_event && expected_postquery_event && unexpected_connectionfailed_event\nWe're using the 'json' Ruby gem to parse the results of the pre and post query curl searches that we previously saved to adequately named files (first 10 lines). Lines #12 to #14 state our expectations for the test result (i.e. the log stream should contain a single DatabasePreQuery, a single DatabasePostQuery and no DatabaseConnectionFailed hits or events). The last line is the actual test (Ruby will return true if all of our expectations were correct or false otherwise).\nMore complicated tests (or analysis of incidents) might, for instance, require searching all Database events in a given time period, counting how many queries, failures, etc. The approach, however, is the same as described above, only slightly more complicated code for iterating over a larger JSON response.\nHere's an example of a curl query for that class of tests (you can find it on Github as well):\n$ curl -XGET 'http://localhost:9200/_search?q=message:Database*&pretty' -d '\n{\n\"query\" : {\n\"range\" : {\n\"@timestamp\" : {\n\"gte\" : \"now-10m/m\"\n}\n}\n}\n}'\nWe can also trace a single path of execution through the system using a correlation ID that we inject into the system when the activity is first initiated: a user clicking on a button, or a batch job starting, for instance. As long as the correlation ID is sufficiently unique for our searches in the log aggregation tool, we will see only results that relate to that query.\nBy searching for the correlation ID, we can determine exactly which servers or containers were involved in the processing of the requests and in effect reconstruct the request journey. There are commercial tools that provide features like this but by building in a few of these features ourselves, we gain a valuable insight into the operation of the system.\nUser Stories for testing logging\nLet\u2019s say we have a browser-based system for the legal market that allows legal professionals to edit and save documents. The document data is saved to a document store database and a message is placed on a message queue. However, as part of the regulatory regime in one country, we need to ensure that the document meets certain requirements, so an \u2018audit\u2019 service listens for the messages on the message queue and then inspects the newly-updated document:\nHere we have an asynchronous, distributed system where apparent success in the browser app \u2013 the document was successfully updated \u2013 might actually need further workflow to be initiated (say, if the document audit finds problems with the document).\nBy using log aggregation with event IDs and transaction tracing, we gain the ability to assert that certain specific log messages should appear in the log aggregation system based on actions in the main transactional system:\nSpecifically, if we know that the audit service should write a log message with the event ID \u2018AuditRecordCreated\u2019, we can set up a test to search for that ID after the web app user journey has completed:\nWith this ability in place to assert our expectations about system behaviour, we can write behaviour-level tests similar to this:\nGiven I run a scenario as a Lawyer\nAnd I create a document\n[And I wait 5 seconds]\nWhen I search the logs API\nThen I should find a recent entry for \u201cAuditRecordCreated\u201d\nThis means that logging acts as a way for us to expand our testing of distributed systems by being specific about the actions and events that we expect to be logged and searching for these at specific times.\nConclusion\nTroubleshooting distributed, scalable systems (often composed of volatile infrastructure) depends on adequate logging and search facilities. We need to log interesting events in our system and associate them to a given business transaction (such as a parcel delivery) via a unique (correlation) ID. Log aggregation and search tools allow us to trace a business transaction end-to-end with a simple ID query. We can also query a category of events to investigate component or system failures (for e.g. find all Database events leading to a incident). Finally, we saw that we can and we should test these operational requirements in a similar way to functional requirements, namely via user stories and BDD scenarios.\nAbout the Authors\nMatthew Skelton has been building, deploying, and operating commercial software systems since 1998. Co-founder and Principal Consultant at Skelton Thatcher Consulting, he specialises in helping organisations to adopt and sustain good practices for building and operating software systems: Continuous Delivery, DevOps, aspects of ITIL, and software operability. Matthew curates the well-known DevOps team topologies patterns and is co-author of the books Database Lifecycle Management (Redgate) and Continuous Delivery with Windows and .NET (O\u2019Reilly). @matthewpskelton\nManuel Pais is a Dev+Build+QA = DevOps advocate (and InfoQ lead editor). People-friendly technologist at Skelton Thatcher Consulting. Jack of all trades, master of continuous improvement, Manuel enjoys helping organizations adopt DevOps, Continuous Delivery and Cloud, from both technical and human perspectives. He has worked on a range of technologies (Java, .Net, web, mobile) and industries (banking, telecom, defense and aviation). @manupaisable\nQuick facts: favourite race: fim da Europa (end of Europe), favourite wine: red, favourite accomplishment: being a dad.",
      "domain": "infoq.com",
      "retrieved_at": "2025-12-15 07:35:44.245060",
      "query_used": "test logging"
    },
    {
      "url": "https://testfixtures.readthedocs.io/en/latest/logging.html",
      "title": "Testing logging - testfixtures 10.0.0 documentation",
      "content": "Testing logging\u00b6\nPython includes a logging\npackage, and while it is widely used, many\npeople assume that logging calls do not need to be tested or find the prospect too daunting.\nTo help with this, testfixtures allows you to easily capture the\noutput of calls to Python\u2019s logging framework and make sure they were\nas expected.\nNote\nThe LogCapture\nclass is useful for checking that\nyour code logs the right messages. If you want to check that\nthe configuration of your handlers is correct, please see\nthe section below.\nMethods of capture\u00b6\nWhen using the tools provided by Testfixtures, there are three different techniques for capturing messages logged to the Python logging framework, depending on the type of test you are writing. They are all described in the sections below.\nThe context manager\u00b6\nThe context manager can be used as follows:\n>>> import logging\n>>> from testfixtures import LogCapture\n>>> with LogCapture() as l:\n... logger = logging.getLogger()\n... logger.info('a message')\n... logger.error('an error')\nFor the duration of the with\nblock, log messages are captured. The\ncontext manager provides a check method that raises an exception if\nthe logging wasn\u2019t as you expected:\n>>> l.check(\n... ('root', 'INFO', 'a message'),\n... ('root', 'ERROR', 'another error'),\n... )\nTraceback (most recent call last):\n...\nAssertionError: sequence not as expected:\nsame:\n(('root', 'INFO', 'a message'),)\nexpected:\n(('root', 'ERROR', 'another error'),)\nactual:\n(('root', 'ERROR', 'an error'),)\nIt also has a string representation that allows you to see what has been logged, which is useful for doc tests:\n>>> print(l)\nroot INFO\na message\nroot ERROR\nan error\nThe decorator\u00b6\nIf you only want to capture logging for a particular test function, you may find the decorator suits your needs better:\nfrom testfixtures import log_capture\n@log_capture()\ndef test_function(capture):\nlogger = logging.getLogger()\nlogger.info('a message')\nlogger.error('an error')\ncapture.check(\n('root', 'INFO', 'a message'),\n('root', 'ERROR', 'an error'),\n)\nNote\nThis method is not compatible with pytest\u2019s fixture discovery stuff.\nInstead, put a fixture such as the following in your conftest.py\n:\nimport pytest\n@pytest.fixture(autouse=True)\ndef capture():\nwith LogCapture() as capture:\nyield capture\nManual usage\u00b6\nIf you want to capture logging for the duration of a doctest or\nin every test in a TestCase\n, then you can use the\nLogCapture\nmanually.\nThe instantiation, which also starts the capturing, is done in the set-up step\nof the TestCase\nor equivalent:\n>>> from testfixtures import LogCapture\n>>> l = LogCapture()\nYou can then execute whatever will log the messages you want to test for:\n>>> from logging import getLogger\n>>> getLogger().info('a message')\nAt any point, you can check what has been logged using the check method:\n>>> l.check(('root', 'INFO', 'a message'))\nAlternatively, you can use the string representation of the\nLogCapture\n:\n>>> print(l)\nroot INFO\na message\nThen, in the tear-down step of the TestCase\nor equivalent,\nyou should make sure you stop the capturing:\n>>> l.uninstall()\nThe uninstall()\nmethod can also be added as an\naddCleanup()\nif that is easier or more compact in your test\nsuite.\nIf you have multiple LogCapture\nobjects in use,\nyou can easily uninstall them all:\n>>> LogCapture.uninstall_all()\nChecking captured log messages\u00b6\nRegardless of how you use the LogCapture\nto\ncapture messages, there are three ways of checking that the messages\ncaptured were as expected.\nThe following example is useful for showing these:\nfrom testfixtures import LogCapture\nfrom logging import getLogger\nlogger = getLogger()\nwith LogCapture() as log:\nlogger.info('start of block number %i', 1)\ntry:\nlogger.debug('inside try block')\nraise RuntimeError('No code to run!')\nexcept:\nlogger.error('error occurred', exc_info=True)\nThe check methods\u00b6\nLogCapture\ninstances have check()\nand check_present()\nmethods to make assertions about\nentries that have been logged.\ncheck()\nwill compare the\nlog messages captured with those you expect. Expected messages are\nexpressed, by default, as three-element tuples where the first element is the name\nof the logger to which the message should have been logged, the\nsecond element is the string representation of the level at which the\nmessage should have been logged and the third element is the message\nthat should have been logged after any parameter interpolation has\ntaken place.\nIf things are as you expected, the method will not raise any exceptions:\n>>> log.check(\n... ('root', 'INFO', 'start of block number 1'),\n... ('root', 'DEBUG', 'inside try block'),\n... ('root', 'ERROR', 'error occurred'),\n... )\nHowever, if the actual messages logged were different, you\u2019ll get an\nAssertionError\nexplaining what happened:\n>>> log.check(('root', 'INFO', 'start of block number 1'))\nTraceback (most recent call last):\n...\nAssertionError: sequence not as expected:\nsame:\n(('root', 'INFO', 'start of block number 1'),)\nexpected:\n()\nactual:\n(('root', 'DEBUG', 'inside try block'), ('root', 'ERROR', 'error occurred'))\nIn contrast, check_present()\nwill only check that the messages you\nspecify are present, and that their order is as specified. Other messages will be ignored:\n>>> log.check_present(\n... ('root', 'INFO', 'start of block number 1'),\n... ('root', 'ERROR', 'error occurred'),\n... )\nIf the order of messages is non-deterministic, then you can be explict that the order doesn\u2019t matter:\n>>> log.check_present(\n... ('root', 'ERROR', 'error occurred'),\n... ('root', 'INFO', 'start of block number 1'),\n... order_matters=False\n... )\nPrinting\u00b6\nThe LogCapture\nhas a string representation that\nshows what messages it has captured. This can be useful in doc tests:\n>>> print(log)\nroot INFO\nstart of block number 1\nroot DEBUG\ninside try block\nroot ERROR\nerror occurred\nThis representation can also be used to check that no logging has occurred:\n>>> empty = LogCapture()\n>>> print(empty)\nNo logging captured\nInspecting\u00b6\nThe LogCapture\nalso keeps a list of the\nLogRecord\ninstances it captures. This is useful when\nyou want to check specifics of the captured logging that aren\u2019t\navailable from either the string representation or the\ncheck()\nmethod.\nA common case of this is where you want to check that exception information was logged for certain messages:\nfrom testfixtures import compare, Comparison as C\ncompare(log.records[-1].exc_info[1], expected=C(RuntimeError('No code to run!')))\nIf you wish the extraction specified in the attributes\nparameter to the\nLogCapture\nconstructor to be taken into account, you can examine the list\nof recorded entries returned by the actual()\nmethod:\nassert log.actual()[-1][-1] == 'error occurred'\nOnly capturing specific logging\u00b6\nSome actions that you want to test may generate a lot of logging, only some of which you actually need to care about.\nThe logging you care about is often only that above a certain log\nlevel. If this is the case, you can configure LogCapture\nto\nonly capture logging at or above a specific level:\n>>> with LogCapture(level=logging.INFO) as l:\n... logger = getLogger()\n... logger.debug('junk')\n... logger.info('something we care about')\n... logger.error('an error')\n>>> print(l)\nroot INFO\nsomething we care about\nroot ERROR\nan error\nIn other cases this problem can be alleviated by only capturing a specific logger:\n>>> with LogCapture('specific') as l:\n... getLogger('something').info('junk')\n... getLogger('specific').info('what we care about')\n... getLogger().info('more junk')\n>>> print(l)\nspecific INFO\nwhat we care about\nHowever, it may be that while you don\u2019t want to capture all logging, you do want to capture logging from multiple specific loggers:\n>>> with LogCapture(('one', 'two')) as l:\n... getLogger('three').info('3')\n... getLogger('two').info('2')\n... getLogger('one').info('1')\n>>> print(l)\ntwo INFO\n2\none INFO\n1\nIt may also be that the simplest thing to do is only capture logging\nfor part of your test. This is particularly common with long doc\ntests. To make this easier, LogCapture\nsupports\nmanual installation and un-installation as shown in the following\nexample:\n>>> l = LogCapture(install=False)\n>>> getLogger().info('junk')\n>>> l.install()\n>>> getLogger().info('something we care about')\n>>> l.uninstall()\n>>> getLogger().info('more junk')\n>>> l.install()\n>>> getLogger().info('something else we care about')\n>>> print(l)\nroot INFO\nsomething we care about\nroot INFO\nsomething else we care about\nOnce you have the filtered to the entries you would like to make assertions about, you may also\nwant to look at a different set of attributes that the defaults for\nLogCapture\n:\n>>> with LogCapture(attributes=('levelname', 'getMessage')) as log:\n... logger = getLogger()\n... logger.debug('a debug message')\n... logger.info('something %s', 'info')\n... logger.error('an error')\n>>> log.check(\n... ('DEBUG', 'a debug message'),\n... ('INFO', 'something info'),\n... ('ERROR', 'an error')\n... )\nAs you can see, if a specified attribute is callable, it will be called and the result used to\nform part of the entry. If you need even more control, you can pass a callable to the\nattributes\nparameter, which can extract any required information from the records and return\nit in the most appropriate form:\ndef extract(record):\nreturn {'level': record.levelname, 'message': record.getMessage()}\n>>> with LogCapture(attributes=extract) as log:\n... logger = getLogger()\n... logger.debug('a debug message')\n... logger.error('an error')\n>>> log.check(\n... {'level': 'DEBUG', 'message': 'a debug message'},\n... {'level': 'ERROR', 'message': 'an error'},\n... )\nChecking the configuration of your log handlers\u00b6\nLogCapture\nis good for checking that your code is logging the\ncorrect messages; just as important is checking that your application\nhas correctly configured log handers. This can be done using a unit\ntest such as the following:\nfrom testfixtures import Comparison as C, compare\nfrom unittest import TestCase\nimport logging\nimport sys\nclass LoggingConfigurationTests(TestCase):\n# We mock out the handlers list for the logger we're\n# configuring in such a way that we have no handlers\n# configured at the start of the test and the handlers our\n# configuration installs are removed at the end of the test.\ndef setUp(self):\nself.logger = logging.getLogger()\nself.orig_handlers = self.logger.handlers\nself.logger.handlers = []\nself.level = self.logger.level\ndef tearDown(self):\nself.logger.handlers = self.orig_handlers\nself.logger.level = self.level\ndef test_basic_configuration(self):\n# Our logging configuration code, in this case just a\n# call to basicConfig:\nlogging.basicConfig(format='%(levelname)s %(message)s',\nlevel=logging.INFO)\n# Now we check the configuration is as expected:\ncompare(self.logger.level, 20)\ncompare([\nC('logging.StreamHandler',\nstream=sys.stderr,\nformatter=C('logging.Formatter',\n_fmt='%(levelname)s %(message)s',\npartial=True),\nlevel=logging.NOTSET,\npartial=True)\n], self.logger.handlers)",
      "domain": "testfixtures.readthedocs.io",
      "retrieved_at": "2025-12-15 07:35:47.102203",
      "query_used": "test logging"
    },
    {
      "url": "https://www.lambdatest.com/learning-hub/test-log",
      "title": "What is a Test Log? Detailed Guide With Best Practices - LambdaTest",
      "content": "Supercharge QA with AI for Faster & Smarter Software Testing\nOVERVIEW\nA test log is a type of test artifact that is generated during test execution. It provides comprehensive information about each test run's success to validate the quality, performance, and functionality of software applications.\nTransparency between the project team and customers plays a vital role in fulfilling their requirements. Therefore, many strategies are created to have zero communication gaps between them. One such strategy is presenting test artifacts to project team members and stakeholders for feedback.\nTest artifacts are essential components of the software testing life cycle (STLC). These artifacts provide numerous benefits, including knowledge sharing with team members, management, and customers to enhance the software product and improve overall communication.\nA test log is a report that summarizes the testing activities performed and the results obtained for a specific software application. The report includes test cases performed, the test results, the test environment, the bugs found, and other information.\nYou can use a test log to enable post-execution debugging of failures and defects related to the product or application. When we talk about test artifacts, it helps to establish transparency among the team members and is appropriately recorded with accurate information. With the help of test artifacts, it becomes easier for the concerned team members to track the changes done in the software and become aware of the latest progress related to testing activities.\nThe following operations you can do when you are working with logs:\nTest Strategy: A high-level document, typically written by the project manager, describes the overall approach to software testing. It is written at the planning stage and outlines testing goals, testing scope, testing methodologies, testing tools, testing infrastructure, and testing resources. It is largely derived from BRS.\nTest Plan: A Test Plan is more detailed than a Test Strategy. A Test Strategy is an outline of the entire project. A Test Plan, on the other hand, is a step-by-step description of a specific project or product testing strategy, testing objectives, testing scope, testing deliverables, testing risk, testing objectives, and testing activities. A test plan is a dynamic, detailed document that contains minute details about the entire testing phase. Its primary purpose is to make sure that testing is done systematically and in an organized way, and that testing activities are aligned with the project's goals and objectives.\nTest Scenario: A Test Scenario is a statement that describes how the software under test performs. It comes from the Use Case statement, which is used to test end-to-end features in linear statements. When testing software, testers must think from the user's point of view. They must test the software to meet the requirements of the users.\nTest Cases: Test case is derived from Test Scenario. A test case is a set of steps or instructions that are used to test a particular feature or function of the software. It is a comprehensive document that includes the name of the test case, the preconditions, the steps, the input data, and the expected outcome. Creating test cases helps in finding issues in the requirement or design of software.\nTraceability Matrix: Traceability Matrix is a document that maps the requirements, design and testing phases of your software development project. A traceability matrix is a table that shows many to many relationships between client requirements and test case. It tracks the development of each requirement and the testing of each requirement to ensure transparency and complete products of software testing.\nTest Log File: A test log file, also referred to as a test log, test execution log, is a log of all the activities performed during software testing. It is a record of everything that happened during the testing process.\nThe log file provides a chronological overview of the testing process. It includes test case execution results and defects encountered. It also includes test environment information and other important information.\nMost test management tools have a built-in ability to create test logs automatically or allow testers to export the test log information in a particular format.\nTest Report: A software test report describes the testing activities and the results of the testing performed on a software application or system. A test report is a formal document that describes the testing process, the results, and any problems that were found during the testing. The testing team or QA team typically prepares the test report and shares it with project managers, developers and clients.\nThe log contains the following statuses for each test run:\nThe logs generated after test execution contain entries that present comprehensive and relevant information about different aspects of the test runs. The following components are part of it.\nThe testing team can follow the test log template standardized as per IEEE Standard 829-1998, a universal standard. The template provides the following details:\nOther information included in the description are: date and time, test environment, case, and procedural specifications.\nEven though creating logs is usually considered a time-consuming activity, it is one of the most essential and beneficial tasks. In addition to tracking testing activities, it allows the team to address various software application issues.\nBelow are a few of the advantages of a logs.\nIn this section, let's try and compare these two based on different parameters.\nTest Log is all about monitoring and tracking critical software testing activities. The primary responsibility lies with the testing team because they know the software life cycle from a testing perspective. Logs are prepared whenever tests are executed, or test scripts are implemented by the team. Its entries can include references to images, files, and other important information related to the application.\nTesters often work with business analysts and product owners to develop high-level test scenarios and review end-to-end test cases. Using this approach, product owners are closer to the users and have a better understanding of how they approach the problem. To ensure that the technically complicated functionality workflows are thoroughly tested, it is better to consult the developers. That is why the testing team needs timely inputs from required stakeholders to update the logs with correct, up-to-date information.\nOrganizations are always looking to hire exceptional software testers who can create an effective and detailed log report. It is always a preferred option to have software testers be technically sound to contribute to the product's success in the long run. Software testers, the chief contributors to the log, need to understand the domain and product requirements.\nSome of the key activities performed by software testers:\nYou need to figure out the overall strategy for dealing with testing log data along with using the logging tools which are best suited for your test environment and the organization's specific testing needs. The best place to begin is to take full advantage of the testing system's built-in logging features.\nSuppose you need logging capabilities that the test system does not provide. In that case, you can consider integrating third-party test logging tools with your testing system to provide the required features and capabilities.\nSome strategies you can adopt in the long run include\nOnce the logs are generated, they should be shared with stakeholders, customers, and the team. Logs provide team members with a comprehensive view of the testing cycle, which can be used to improve the testing process. You can leverage LambdaTest's cloud-based testing platform to share these logs on a daily basis with the team in an automated manner.\nLambdaTest is a continuous quality cloud that gives you the flexibility to perform manual and automated testing of websites and mobile apps across 3000+ real browsers, devices, and OS combinations. It provides an online browser farm and device farm to test your functional and non-functional requirements on real browsers, devices, and OS combinations. It avoids the cumbersome process of maintaining in-house test infrastructure.\nSubscribe to the LambdaTest YouTube channel for software testing tutorials around Selenium testing, Playwright browser testing, Appium, and more.\nFor manual testing, LambdaTest provides a Test Logs library that contains the logs of all your executed real-time browser tests on the platform. It includes real-time tests like\nApart from the manual testing, you can perform end-to-end automation testing for your web and mobile applications. It is done by running automation tests at scale on the test automation cloud, which is highly reliable and scalable. You can run automated tests with Selenium, Cypress, Appium, and more, the LambdaTest cloud-based platform.\nBelow is the log generated on the LambdaTest platform for Selenium Java tests. The status of this test is - PASSED.\nYou can view the test run logs on the LambdaTest automation testing dashboard. For example, Selenium, Console, and Terminal logs.\nClick the test or the build of which you want to see the automation log.\nClick on the Share icon to share the logs with your team members or colleagues.\nEnter the email addresses of your team members or colleagues, provide a message (description) if you have about the test build, and set the link's expiry date. You can also share your automation logs via a shareable link. Once you are done with the details, click Done.\nSign up on LambdaTest and get your 100 minutes of automated testing for FREE.\nIn this tutorial, we discussed how logs are an excellent feature for monitoring the overall system performance. You can rely on the status results of each test as well. It becomes crucial for the testing team to update logs with correct and up-to-date information. There will be project teams you will rely on logs and refer to when the need arises. LambdaTest provides you with in-built log capabilities where you can view your testing results in an automated way.\nThe log provides a historical record of which events occurred during a test run or a scheduled run and the status of each verification point.\nThe logs in testing are one of the artifacts generated during test execution. It provides a comprehensive summary of the overall test runs and specifies the passed and failed tests.\nDid you find this page helpful?",
      "domain": "lambdatest.com",
      "retrieved_at": "2025-12-15 07:35:49.399039",
      "query_used": "test logging"
    },
    {
      "url": "https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html",
      "title": "Logging - OWASP Cheat Sheet Series",
      "content": "Logging Cheat Sheet\u00b6\nIntroduction\u00b6\nThis cheat sheet is focused on providing developers with concentrated guidance on building application logging mechanisms, especially related to security logging.\nMany systems enable network device, operating system, web server, mail server and database server logging, but often custom application event logging is missing, disabled or poorly configured. It provides much greater insight than infrastructure logging alone. Web application (e.g. web site or web service) logging is much more than having web server logs enabled (e.g. using Extended Log File Format).\nApplication logging should be consistent within the application, consistent across an organization's application portfolio and use industry standards where relevant, so the logged event data can be consumed, correlated, analyzed and managed by a wide variety of systems.\nPurpose\u00b6\nApplication logging should always be included for security events. Application logs are invaluable data for both security and operational use cases.\nOperational use cases\u00b6\n- General debugging\n- Establishing baselines\n- Business process monitoring e.g. sales process abandonment, transactions, connections\n- Providing information about problems and unusual conditions\n- Performance monitoring e.g. data load time, page timeouts\n- Other business-specific requirements\nSecurity use cases\u00b6\nApplication logging might also be used to record other types of events too such as:\n- Anti-automation monitoring\n- Identifying security incidents\n- Monitoring policy violations\n- Assisting non-repudiation controls (note that the trait non-repudiation is hard to achieve for logs because their trustworthiness is often just based on the logging party being audited properly while mechanisms like digital signatures are hard to utilize here)\n- Audit trails e.g. data addition, modification and deletion, data exports\n- Compliance monitoring\n- Data for subsequent requests for information e.g. data subject access, freedom of information, litigation, police and other regulatory investigations\n- Legally sanctioned interception of data e.g. application-layer wire-tapping\n- Contributing additional application-specific data for incident investigation which is lacking in other log sources\n- Helping defend against vulnerability identification and exploitation through attack detection\nProcess monitoring, audit, and transaction logs/trails etc. are usually collected for different purposes than security event logging, and this often means they should be kept separate.\nThe types of events and details collected will tend to be different.\nFor example a PCIDSS audit log will contain a chronological record of activities to provide an independently verifiable trail that permits reconstruction, review and examination to determine the original sequence of attributable transactions. It is important not to log too much, or too little.\nUse knowledge of the intended purposes to guide what, when and how much. The remainder of this cheat sheet primarily discusses security event logging.\nDesign, implementation, and testing\u00b6\nEvent data sources\u00b6\nThe application itself has access to a wide range of information events that should be used to generate log entries. Thus, the primary event data source is the application code itself.\nThe application has the most information about the user (e.g. identity, roles, permissions) and the context of the event (target, action, outcomes), and often this data is not available to either infrastructure devices, or even closely-related applications.\nOther sources of information about application usage that could also be considered are:\n- Client software e.g. actions on desktop software and mobile devices in local logs or using messaging technologies, JavaScript exception handler via AJAX, web browser such as using Content Security Policy (CSP) reporting mechanism\n- Embedded instrumentation code\n- Network firewalls\n- Network and host intrusion detection systems (NIDS and HIDS)\n- Closely-related applications e.g. filters built into web server software, web server URL redirects/rewrites to scripted custom error pages and handlers\n- Application firewalls e.g. filters, guards, XML gateways, database firewalls, web application firewalls (WAFs)\n- Database applications e.g. automatic audit trails, trigger-based actions\n- Reputation monitoring services e.g. uptime or malware monitoring\n- Other applications e.g. fraud monitoring, CRM\n- Operating system e.g. mobile platform\nThe degree of confidence in the event information has to be considered when including event data from systems in a different trust zone. Data may be missing, modified, forged, replayed and could be malicious \u2013 it must always be treated as untrusted data.\nConsider how the source can be verified, and how integrity and non-repudiation can be enforced.\nWhere to record event data\u00b6\nApplications commonly write event log data to the file system or a database (SQL or NoSQL). Applications installed on desktops and on mobile devices may use local storage and local databases, as well as sending data to remote storage.\nYour selected framework may limit the available choices. All types of applications may send event data to remote systems (instead of or as well as more local storage).\nThis could be a centralized log collection and management system (e.g. SIEM or SEM) or another application elsewhere. Consider whether the application can simply send its event stream, unbuffered, to stdout, for management by the execution environment.\n- When using the file system, it is preferable to use a separate partition than those used by the operating system, other application files and user generated content\n- For file-based logs, apply strict permissions concerning which users can access the directories, and the permissions of files within the directories\n- In web applications, the logs should not be exposed in web-accessible locations, and if done so, should have restricted access and be configured with a plain text MIME type (not HTML)\n- When using a database, it is preferable to utilize a separate database account that is only used for writing log data and which has very restrictive database, table, function and command permissions\n- Use standard formats over secure protocols to record and send event data, or log files, to other systems e.g. Common Log File System (CLFS) or Common Event Format (CEF) over syslog; standard formats facilitate integration with centralised logging services\nConsider separate files/tables for extended event information such as error stack traces or a record of HTTP request and response headers and bodies.\nWhich events to log\u00b6\nThe level and content of security monitoring, alerting, and reporting needs to be set during the requirements and design stage of projects, and should be proportionate to the information security risks. This can then be used to define what should be logged.\nThere is no one size fits all solution, and a blind checklist approach can lead to unnecessary \"alarm fog\" that means real problems go undetected.\nWhere possible, always log:\n- Input validation failures e.g. protocol violations, unacceptable encodings, invalid parameter names and values\n- A specific event for failures to validate a value against a discrete and finite list of valid values (e.g. a country from a dropdown). This is a high security event as it can only be attack activity. For example\ninput_validation_fail[:field,userid]\n.\n- A specific event for failures to validate a value against a discrete and finite list of valid values (e.g. a country from a dropdown). This is a high security event as it can only be attack activity. For example\n- Output validation failures e.g. database record set mismatch, invalid data encoding\n- Authentication successes and failures\n- Authorization (access control) failures\n- Session management failures e.g. cookie session identification value modification or suspicious JWT validation failures\n- Application errors and system events e.g. syntax and runtime errors, connectivity problems, performance issues, third party service error messages, file system errors, file upload virus detection, configuration changes\n- Application and related systems start-ups and shut-downs, and logging initialization (starting, stopping or pausing)\n- Use of higher-risk functionality including:\n- User administration actions such as addition or deletion of users, changes to privileges, assigning users to tokens, adding or deleting tokens\n- Use of systems administrative privileges or access by application administrators including all actions by those users\n- Use of default or shared accounts or a \"break-glass\" account.\n- Access to sensitive data such as payment cardholder data,\n- Encryption activities such as use or rotation of cryptographic keys\n- Creation and deletion of system-level objects\n- Data import and export including screen-based reports\n- Submission and processing of user-generated content - especially file uploads\n- Deserialization failures\n- Network connections and associated failures such as backend TLS failures (including certificate validation failures), or requests with an unexpected HTTP verb\n- Legal and other opt-ins e.g. permissions for mobile phone capabilities, terms of use, terms & conditions, personal data usage consent, permission to receive marketing communications\n- Suspicious business logic activities such as:\n- Attempts to perform a set actions out of order/bypass flow control\n- Actions which don't make sense in the business context\n- Attempts to exceed limitations for particular actions\nOptionally consider if the following events can be logged and whether it is desirable information:\n- Sequencing failure\n- Excessive use\n- Data changes\n- Fraud and other criminal activities\n- Suspicious, unacceptable, or unexpected behavior\n- Modifications to configuration\n- Application code file and/or memory changes\nEvent attributes\u00b6\nEach log entry needs to include sufficient information for the intended subsequent monitoring and analysis. It could be full content data, but is more likely to be an extract or just summary properties.\nThe application logs must record \"when, where, who and what\" for each event.\nThe properties for these will be different depending on the architecture, class of application and host system/device, but often include the following:\n- When\n- Log date and time (international format)\n- Event date and time - the event timestamp may be different to the time of logging e.g. server logging where the client application is hosted on remote device that is only periodically or intermittently online\n- Interaction identifier\nNote A\n- Where\n- Application identifier e.g. name and version\n- Application address e.g. cluster/hostname or server IPv4 or IPv6 address and port number, workstation identity, local device identifier\n- Service e.g. name and protocol\n- Geolocation\n- Window/form/page e.g. entry point URL and HTTP method for a web application, dialogue box name\n- Code location e.g. script name, module name\n- Who (human or machine user)\n- Source address e.g. user's device/machine identifier, user's IP address, cell/RF tower ID, mobile telephone number\n- User identity (if authenticated or otherwise known) e.g. user database table primary key-value, username, license number\n- What\n- Type of event\nNote B\n- Severity of event\nNote B\ne.g.{0=emergency, 1=alert, ..., 7=debug}, {fatal, error, warning, info, debug, trace}\n- Security relevant event flag (if the logs contain non-security event data too)\n- Description\n- Type of event\nAdditionally consider recording:\n- Secondary time source (e.g. GPS) event date and time\n- Action - original intended purpose of the request e.g. Log in, Refresh session ID, Log out, Update profile\n- Object e.g. the affected component or other object (user account, data resource, file) e.g. URL, Session ID, User account, File\n- Result status - whether the ACTION aimed at the OBJECT was successful e.g. Success, Fail, Defer\n- Reason - why the status above occurred e.g. User not authenticated in database check ..., Incorrect credentials\n- HTTP Status Code (web applications only) - the status code returned to the user (often 200 or 301)\n- Request HTTP headers or HTTP User Agent (web applications only)\n- User type classification e.g. public, authenticated user, CMS user, search engine, authorized penetration tester, uptime monitor (see \"Data to exclude\" below)\n- Analytical confidence in the event detection\nNote B\ne.g. low, medium, high or a numeric value - Responses seen by the user and/or taken by the application e.g. status code, custom text messages, session termination, administrator alerts\n- Extended details e.g. stack trace, system error messages, debug information, HTTP request body, HTTP response headers and body\n- Internal classifications e.g. responsibility, compliance references\n- External classifications e.g. NIST Security Content Automation Protocol (SCAP), Mitre Common Attack Pattern Enumeration and Classification (CAPEC)\nFor more information on these, see the \"other\" related articles listed at the end, especially the comprehensive article by Anton Chuvakin and Gunnar Peterson.\nNote A: The \"Interaction identifier\" is a method of linking all (relevant) events for a single user interaction (e.g. desktop application form submission, web page request, mobile app button click, web service call). The application knows all these events relate to the same interaction, and this should be recorded instead of losing the information and forcing subsequent correlation techniques to re-construct the separate events. For example, a single SOAP request may have multiple input validation failures and they may span a small range of times. As another example, an output validation failure may occur much later than the input submission for a long-running \"saga request\" submitted by the application to a database server.\nNote B: Each organisation should ensure it has a consistent, and documented, approach to classification of events (type, confidence, severity), the syntax of descriptions, and field lengths and data types including the format used for dates/times.\nData to exclude\u00b6\nNever log data unless it is legally sanctioned. For example, intercepting some communications, monitoring employees, and collecting some data without consent may all be illegal.\nNever exclude any events from \"known\" users such as other internal systems, \"trusted\" third parties, search engine robots, uptime/process and other remote monitoring systems, pen testers, auditors. However, you may want to include a classification flag for each of these in the recorded data.\nThe following should usually not be recorded directly in the logs, but instead should be removed, masked, sanitized, hashed, or encrypted:\n- Application source code\n- Session identification values (consider replacing with a hashed value if needed to track session specific events)\n- Access tokens\n- Sensitive personal data and some forms of personally identifiable information (PII) e.g. health, government identifiers, vulnerable people\n- Authentication passwords\n- Database connection strings\n- Encryption keys and other primary secrets\n- Bank account or payment card holder data\n- Data of a higher security classification than the logging system is allowed to store\n- Commercially-sensitive information\n- Information it is illegal to collect in the relevant jurisdictions\n- Information a user has opted out of collection, or not consented to e.g. use of do not track, or where consent to collect has expired\nSometimes the following data can also exist, and whilst useful for subsequent investigation, it may also need to be treated in some special manner before the event is recorded:\n- File paths\n- Database connection strings\n- Internal network names and addresses\n- Non sensitive personal data (e.g. personal names, telephone numbers, email addresses)\nConsider using personal data de-identification techniques such as deletion, scrambling or pseudonymization of direct and indirect identifiers where the individual's identity is not required, or the risk is considered too great.\nIn some systems, sanitization can be undertaken post log collection, and prior to log display.\nCustomizable logging\u00b6\nIt may be desirable to be able to alter the level of logging (type of events based on severity or threat level, amount of detail recorded). If this is implemented, ensure that:\n- The default level must provide sufficient detail for business needs\n- It should not be possible to completely deactivate application logging or logging of events that are necessary for compliance requirements\n- Alterations to the level/extent of logging must be intrinsic to the application (e.g. undertaken automatically by the application based on an approved algorithm) or follow change management processes (e.g. changes to configuration data, modification of source code)\n- The logging level must be verified periodically\nEvent collection\u00b6\nIf your development framework supports suitable logging mechanisms, use or build upon that. Otherwise, implement an application-wide log handler which can be called from other modules/components.\nDocument the interface referencing the organisation-specific event classification and description syntax requirements.\nIf possible create this log handler as a standard module that can be thoroughly tested, deployed in multiple applications, and added to a list of approved and recommended modules.\n- Perform input validation on event data from other trust zones to ensure it is in the correct format (and consider alerting and not logging if there is an input validation failure)\n- Perform sanitization on all event data to prevent log injection attacks e.g. carriage return (CR), line feed (LF) and delimiter characters (and optionally to remove sensitive data)\n- Encode data correctly for the output (logged) format\n- If writing to databases, read, understand, and apply the SQL injection cheat sheet\n- Ensure failures in the logging processes/systems do not prevent the application from otherwise running or allow information leakage\n- Synchronize time across all servers and devices\nNote C\nNote C: This is not always possible where the application is running on a device under some other party's control (e.g. on an individual's mobile phone, on a remote customer's workstation which is on another corporate network). In these cases, attempt to measure the time offset, or record a confidence level in the event timestamp.\nWhere possible, record data in a standard format, or at least ensure it can be exported/broadcast using an industry-standard format.\nIn some cases, events may be relayed or collected together in intermediate points. In the latter some data may be aggregated or summarized before forwarding on to a central repository and analysis system.\nVerification\u00b6\nLogging functionality and systems must be included in code review, application testing and security verification processes:\n- Ensure the logging is working correctly and as specified\n- Check that events are being classified consistently and the field names, types and lengths are correctly defined to an agreed standard\n- Ensure logging is implemented and enabled during application security, fuzz, penetration, and performance testing\n- Test the mechanisms are not susceptible to injection attacks\n- Ensure there are no unwanted side-effects when logging occurs\n- Check the effect on the logging mechanisms when external network connectivity is lost (if this is usually required)\n- Ensure logging cannot be used to deplete system resources, for example by filling up disk space or exceeding database transaction log space, leading to denial of service\n- Test the effect on the application of logging failures such as simulated database connectivity loss, lack of file system space, missing write permissions to the file system, and runtime errors in the logging module itself\n- Verify access controls on the event log data\n- If log data is utilized in any action against users (e.g. blocking access, account lock-out), ensure this cannot be used to cause denial of service (DoS) of other users\nNetwork architecture\u00b6\nAs an example, the diagram below shows a service that provides business functionality to customers. We recommend creating a centralized system for collecting logs. There may be many such services, but all of them must securely collect logs in a centralized system.\nApplications of this business service are located in network segments:\n- FRONTEND 1 aka DMZ (UI)\n- MIDDLEWARE 1 (business application - service core)\n- BACKEND 1 (service database)\nThe service responsible for collecting IT events, including security events, is located in the following segments:\n- BACKEND 2 (log storage)\n- MIDDLEWARE 3 - 2 applications:\n- log loader application that download log from storage, pre-processes, and transfer to UI\n- log collector that accepts logs from business applications, other infrastructure, cloud applications and saves in log storage\n- FRONTEND 2 (UI for viewing business service event logs)\n- FRONTEND 3 (applications that receive logs from cloud applications and transfer logs to log collector)\n- It is allowed to combine the functionality of two applications in one\nFor example, all external requests from users go through the API management service, see application in MIDDLEWARE 2 segment.\nAs you can see in the image above, at the network level, the processes of saving and downloading logs require opening different network accesses (ports), arrows are highlighted in different colors. Also, saving and downloading are performed by different applications.\nFull network segmentation cheat sheet by sergiomarotco: link\nDeployment and operation\u00b6\nRelease\u00b6\n- Provide security configuration information by adding details about the logging mechanisms to release documentation\n- Brief the application/process owner about the application logging mechanisms\n- Ensure the outputs of the monitoring (see below) are integrated with incident response processes\nOperation\u00b6\nEnable processes to detect whether logging has stopped, and to identify tampering or unauthorized access and deletion (see protection below).\nProtection\u00b6\nThe logging mechanisms and collected event data must be protected from mis-use such as tampering in transit, and unauthorized access, modification and deletion once stored. Logs may contain personal and other sensitive information, or the data may contain information regarding the application's code and logic.\nIn addition, the collected information in the logs may itself have business value (to competitors, gossip-mongers, journalists and activists) such as allowing the estimate of revenues, or providing performance information about employees.\nThis data may be held on end devices, at intermediate points, in centralized repositories and in archives and backups.\nConsider whether parts of the data may need to be excluded, masked, sanitized, hashed, or encrypted during examination or extraction.\nAt rest:\n- Build in tamper detection so you know if a record has been modified or deleted\n- Store or copy log data to read-only media as soon as possible\n- All access to the logs must be recorded and monitored (and may need prior approval)\n- The privileges to read log data should be restricted and reviewed periodically\nIn transit:\n- If log data is sent over untrusted networks (e.g. for collection, for dispatch elsewhere, for analysis, for reporting), use a secure transmission protocol\n- Consider whether the origin of the event data needs to be verified\n- Perform due diligence checks (regulatory and security) before sending event data to third parties\nSee NIST SP 800-92\nGuide to Computer Security Log Management for more guidance.\nMonitoring of events\u00b6\nThe logged event data needs to be available to review and there are processes in place for appropriate monitoring, alerting, and reporting:\n- Incorporate the application logging into any existing log management systems/infrastructure e.g. centralized logging and analysis systems\n- Ensure event information is available to appropriate teams\n- Enable alerting and signal the responsible teams about more serious events immediately\n- Share relevant event information with other detection systems, to related organizations and centralized intelligence gathering/sharing systems\nDisposal of logs\u00b6\nLog data, temporary debug logs, and backups/copies/extractions, must not be destroyed before the duration of the required data retention period, and must not be kept beyond this time.\nLegal, regulatory and contractual obligations may impact on these periods.\nAttacks on Logs\u00b6\nBecause of their usefulness as a defense, logs may be a target of attacks. See also OWASP Log Injection and CWE-117.\nConfidentiality\u00b6\nWho should be able to read what? A confidentiality attack enables an unauthorized party to access sensitive information stored in logs.\n- Logs contain PII of users. Attackers gather PII, then either release it or use it as a stepping stone for further attacks on those users.\n- Logs contain technical secrets such as passwords. Attackers use it as a stepping stone for deeper attacks.\nIntegrity\u00b6\nWhich information should be modifiable by whom?\n- An attacker with read access to a log uses it to exfiltrate secrets.\n- An attack leverages logs to connect with exploitable facets of logging platforms, such as sending in a payload over syslog in order to cause an out-of-bounds write.\nAvailability\u00b6\nWhat downtime is acceptable?\n- An attacker floods log files in order to exhaust disk space available for non-logging facets of system functioning. For example, the same disk used for log files might be used for SQL storage of application data.\n- An attacker floods log files in order to exhaust disk space available for further logging.\n- An attacker uses one log entry to destroy other log entries.\n- An attacker leverages poor performance of logging code to reduce application performance\nAccountability\u00b6\nWho is responsible for harm?\n- An attacker prevent writes in order to cover their tracks.\n- An attacker prevent damages the log in order to cover their tracks.\n- An attacker causes the wrong identity to be logged in order to conceal the responsible party.\nRelated articles\u00b6\n- OWASP ESAPI Documentation.\n- OWASP Logging Project.\n- IETF syslog protocol.\n- Mitre Common Event Expression (CEE) (as of 2014 no longer actively developed).\n- NIST SP 800-92 Guide to Computer Security Log Management.\n- PCISSC PCI DSS v2.0 Requirement 10 and PA-DSS v2.0 Requirement 4.\n- W3C Extended Log File Format.\n- Other Build Visibility In, Richard Bejtlich, TaoSecurity blog.\n- Other Common Event Format (CEF), Arcsight.\n- Other Log Event Extended Format (LEEF), IBM.\n- Other Common Log File System (CLFS), Microsoft.\n- Other Building Secure Applications: Consistent Logging, Rohit Sethi & Nish Bhalla, Symantec Connect.",
      "domain": "cheatsheetseries.owasp.org",
      "retrieved_at": "2025-12-15 07:35:50.462960",
      "query_used": "test logging"
    },
    {
      "url": "https://docs.gradle.org/current/dsl/org.gradle.api.tasks.testing.logging.TestLogging.html",
      "title": "TestLogging - Gradle DSL Version 9.2.1",
      "content": "Table of Contents\nOptions that determine which test events get logged, and at which detail.\nThe display granularity of the events to be logged. For example, if set to 0, a method-level event will be displayed as \"Test Run > Test Worker x > org.SomeClass > org.someMethod\". If set to 2, the same event will be displayed as \"org.someClass > org.someMethod\".\n-1 denotes the highest granularity and corresponds to an atomic test.\nSet\n<TestLogEvent\n>\nevents\nSet\n<TestLogEvent\n>The events to be logged.\nTestExceptionFormat\nexceptionFormat\nThe format to be used for logging test exceptions. Only relevant if showStackTraces\nis true\n. Defaults to TestExceptionFormat.FULL\nfor\nthe INFO and DEBUG log levels and TestExceptionFormat.SHORT\nfor the LIFECYCLE log level.\nThe maximum granularity of the events to be logged. Typically, 0 corresponds to the Gradle-generated test suite for the whole test run, 1 corresponds to the Gradle-generated test suite for a particular test JVM, 2 corresponds to a test class, and 3 corresponds to a test method. These values may extend higher if user-defined suites or parameterized test methods are executed. Events from levels higher than the specified granularity will be ignored.\nThe default granularity is -1, which specifies that test events from only the most granular level should be logged. Setting this value to something lower will cause events from a higher level to be ignored. For example, setting the value to 3 will cause only events from the test method level to be logged and any events from iterations of a parameterized test method will be ignored.\nThe minimum granularity of the events to be logged. Typically, 0 corresponds to events from the Gradle-generated test suite for the whole test run, 1 corresponds to the Gradle-generated test suite for a particular test JVM, 2 corresponds to a test class, and 3 corresponds to a test method. These values may extend higher if user-defined suites or parameterized test methods are executed. Events from levels lower than the specified granularity will be ignored.\nThe default granularity is -1, which specifies that test events from only the most granular level should be logged. In other words, if a test method is not parameterized, only events from the test method will be logged and events from the test class and lower will be ignored. On the other hand, if a test method is parameterized, then events from the iterations of that test method will be logged and events from the test method and lower will be ignored.\nTells whether causes of exceptions that occur during test execution will be logged. Only relevant if showExceptions\nis true\n. Defaults to true.\nTells whether exceptions that occur during test execution will be logged. Typically these exceptions coincide with a \"failed\" event. Defaults to true.\nTells whether stack traces of exceptions that occur during test execution will be logged. Defaults to true.\nTells whether output on standard out and standard error will be logged. Equivalent to checking if both log events TestLogEvent.STANDARD_OUT\nand TestLogEvent.STANDARD_ERROR\nare\nset.\nSet\n<TestStackTraceFilter\n>\nstackTraceFilters\nSet\n<TestStackTraceFilter\n>The set of filters to be used for sanitizing test stack traces.",
      "domain": "docs.gradle.org",
      "retrieved_at": "2025-12-15 07:35:51.728969",
      "query_used": "test logging"
    },
    {
      "url": "https://www.browserstack.com/guide/what-is-test-log",
      "title": "What is a Test Log? | BrowserStack",
      "content": "Test logging is a critical part of the Software Testing Life Cycle (STLC). A test log records every detail of test execution\u2014results, environment, defects, and root cause analysis (RCA)\u2014to support reporting, traceability, and collaboration.\nOverview\nWhat is a Test Log?\n- A chronological record of test case execution, environment details, and outcomes.\n- Helps QAs diagnose failures and verify software quality standards.\nImportance of Test Logs in the Testing Process\n- Ensures traceability of testing activities across iterations.\n- Aids defect tracking with severity and priority.\n- Acts as an audit trail for compliance-heavy industries.\n- Promotes collaboration and transparency across teams.\nCore Components Included in a Test Log File\n- Unique test case ID and description.\n- Test environment details and input data.\n- Execution results (Pass/Fail/Skipped).\n- Screenshots, logs, and error messages for failed runs.\n- Comments and observations for context.\nHow to Create, Analyze, and Share Test Logs Effectively\n- Create: Capture execution details during test runs.\n- Analyze: Review logs to detect patterns, errors, and process gaps.\n- Share: Distribute via collaboration tools (Slack, Confluence) or test management systems (Jira, TestRail).\n- Store: Maintain historical logs for audits, debugging, and future releases.\nThis guide covers the definition, components, advantages, creation, analysis, sharing, and storage of test logs with practical examples.\nWhat is a Test Log?\nTest logging refers to the process of documenting the details of the testing process, including the test case execution results, the environment configuration, and the issues encountered during testing with proper RCA.\nThe prime purpose of test logging is to keep a record of the testing process so that the testing team can review the results and determine whether the software under test meets the expected quality standards. The focus is to enable post execution diagnosis of failures and defects in the software.\nWhat are testing artifacts?\nTest artifacts are nothing but a set of documents that are created or generated while performing testing. Test artifacts are required to develop transparency between the project team and client to avoid the communication gap and maintain a healthy relation. These test artifacts are shared within the team, clients, product managers, project managers, team leaders, and other stakeholders associated with the project. Due to this transparency, it becomes easy to identify and track changes and also be aware of recent progress of activities of testing from requirement.\nBelow are 7 main test artifacts used in Software Testing:\n1. Test Strategy: Test Strategy is a high-level document mostly developed by the project manager that outlines the overall approach to be taken for software testing. This document is created at the planning phase that defines testing objectives, test scope, methodologies, tools, infrastructure, and test resources to be used. It is mostly derived from Business Requirement Specification (BRS).\n2. Test Plan: While, Test Strategy is just an outline for the whole project, a Test Plan is a detailed document which outlines the testing strategy, objectives, scope, test deliverables, risk, objectives, and activities for a particular project or product. It is a dynamic document that has minute details about how the whole testing phase will work. The main purpose of a test plan is to ensure that testing is conducted in a systematic and organised manner, and that the testing activities align with the project goals and objectives.\n3. Test Scenario: Test scenario is a statement used to describe the functionality of the software under test. It is derived from Use Case which is used to validate end to end feature testing in linear statements. Testers have to think from the customer\u2019s perspective to test the software to ensure it meets the needs of the users.\nAlso Read: Use Case vs Test Case: Core Differences\n4. Test Cases: Test Cases are derived from Test Scenarios and are a set of instructions or steps that are designed to verify a specific functionality or feature of the software under test. It is a detailed document consisting of test case name, preconditions, steps, input data, and expected result. Developing test cases helps in identifying problems in requirement or design of the software.\nRead More: Test Plan vs Test Case: Core Differences\n5. Traceability Matrix: It is a document that provides a mapping between the requirements, design, and testing phases of a software development project. It is a matrix table that visualises many to many relationships among client requirements and test cases. It is used to track the development and testing of each requirement. It helps to ensure transparency and completeness of products of software testing.\n6. Test Log File: Test log file also known as a test log or test execution log, is a record of the activities performed during the execution of software testing. It provides a chronological account of the testing process, including details such as test case execution results, defects encountered, test environment information, and other relevant information.\nTest management tools often provide built-in capabilities to generate test logs automatically or enable testers to export the log information in a specific format.\nThe test log file serves as a valuable source of information for test reporting, tracking progress, identifying trends, and analyzing the overall quality of the software under test. It can also facilitate collaboration among team members and provide a historical record for future reference or audits.\n7. Software Test Report: A software test report is a document that provides a comprehensive overview of the testing activities and results performed on a software system or application. It serves as a formal record of the testing process, outcomes, and any issues encountered during testing. The test report is typically prepared by the testing team or quality assurance (QA) team and shared with relevant stakeholders, such as project managers, developers, and clients.\nWhy is test log necessary?\nCreating test log is a time-consuming activity however it is essential to create them for the following reasons:\n- It can be used as a test record as it captures a detailed record of the testing process, including what was tested, who tested it, when it was tested, and the results of the test cycle. This can be used as a reference or to assist with debugging.\n- It can be used to troubleshoot issues that arise during the testing process. One can identify the root cause of issues and take steps accordingly to address them by reviewing the test logs.\n- It provides an audit trail which is important for compliance purposes. For e.g: In healthcare and finance sectors it is necessary to have a detailed record of the testing process to ensure that all requirements have been met and the software is compliant with required standards.\n- It promotes collaboration as test logs can be shared with the cross functional teams. This ensures that everyone is on the same page and there is no communication gap.\nAdvantages of Test Log\n- Traceability: All the test activities and results can be tracked with a test log. This helps to ensure that all the required test activities were performed and completed for a particular iteration as a proof to show the required audience whenever it is demanded. Also, any issues identified during testing can be accurately traced back to their source.\n- Defect tracking: Issues found during testing can be tracked with details such as severity, priority and status. This helps the QA team to prioritise the issues which need early resolution before release.\n- Documentation: Test artifacts and deliverables are the fundamental part of the Software Testing Lifecycle (STLC). A test log consists of all the necessary test execution details which can be used as a formal record. It also helps to provide a historical record of the testing process which can be referred to in future.\n- Transparency between teams: A test log facilitates communication between QA members as well as with the development and product team. For example- Test logs can be uploaded on Confluence or Sharepoint and can serve as a common platform which can be referred by any team to view test logs.\n- Test analysis and improvement: The historical test log data can be reviewed by the testing team to identify trends (w.r.t issues), patterns, and areas for improvement. This helps to enhance the testing process and upgrade the quality of software/ application under test.\nRead More: Defect Management in Software Testing\nVarious Components of Test Log\nTest logs created after every test execution should have a trail of entries that should precisely describe the execution activities in detail. Below are the components of a typical test log however this may vary depending on the testing approach or tool used.\n- Test case ID: A unique identifier for the test case being executed.\n- Test case description: A short summary of the test case being executed.\n- Test environment: The hardware and software environment details in which the test case was executed.\n- Test data: The input data used for a test execution.\n- Test result: The result of test execution as Passed/ Failed/ Skipped.\n- Date and Time of execution: Date and time when the test case was executed.\n- Tester\u2019s name: Name of the tester who executed the test case.\n- Test logs: Relevant test logs, error messages, crash logs, failure screenshots generated during test execution.\n- Test artifacts: Any artifacts used during test execution such as System Requirement Specifications (SRS)/ Business Requirement Specifications (BRS)\n- Comments: Any comments or observations about the test execution.\nTest Log Template\nHere\u2019s how a test log template looks like\nHow to create a Test Log\nA test log is created mainly by the software testers, quality assurance engineers, or other testing professionals. Below are some general steps to create a test log.\n- Identify the test cases and create a list of these test cases.\n- Decide among the team and define the components that should be included in a test log such as test case id, test case description, date and time of execution, tested by, etc.\n- Execute the test cases and record the results along with relevant logs and screenshots.\n- Review the test log and identify any issues and use this to improve the testing process.\nIt is essential to share the test logs with the cross functional teams and the stakeholders as needed and maintain the test log on some shared drive.\nHow to analyse Test Log\nAnalysing a test log helps in identifying issues that may need to be addressed. Below are some common steps to analyse a test log.\n- Review the test results to identify any issues or areas of concern.\n- Review each test case to identify specific issues or areas of improvement. For e.g: A particular feature was restructured however the test case was not modified accordingly and hence the test case failed/ skipped.\n- Analyse the test logs to look for error messages or warnings that may help to find the cause of the problem.\n- Identify root causes by gathering the information from the test log and performing some additional testing and debugging.\n- Based on the analysis of the test log, give recommendations to alter the software testing process by providing compelling reasons and benefits to the test system by adopting this change.\n- After the recommendation is approved by the team collectively, update the test log with new information or analysis. Finally, circulate the updated test log to the cross functional teams so that everyone gets well acquainted with the updated test log.\nTest Log Report\nA test log report is a document that provides a summary of the testing activities and results for a particular software application. This report contains information such as test cases executed, test results, test environment, bugs encountered, and any other relevant data.\nTest log report caters to provide a historical record of the testing process which can be used by the testing, development, product team and other stakeholders.\nBelow are some of the general elements of a test log report:\n- List of test cases executed.\n- Test results with Passed/ Failed/ Skipped/ NA status.\n- Severity and Priority of test result.\n- Any supporting data or logs or screenshots captured for failed test cases.\n- Test environment details. (Software and hardware used for test execution)\nHow to share a Test Log with the team?\nDepending on the organisation\u2019s preference and tools used, one can share test logs by following common methods:\n- The simplest way to share a test log is via email.\n- Test logs can also be shared via collaborative tools such as Microsoft Teams, Slack, and SharePoint.\n- Any version control system such as Git, SVN, Bitbucket can be used to share the test log. Dump all the information on a shared repository and share with the team. This ensures that everyone has access to the same version of the report.\n- Test Management tools can also be used to share the test log such as Jira, Confluence. By using tools such as Confluence one can keep all the testing related information in one place and ensure that all the team members are referring to the latest version of the test log.BrowserStack\u2019s Test Management Tool allows you to integrate with Jira and write and manage your test cases and test log files for a streamlined testing process.\nTry BrowserStack Test Management Tool for Free\nRoles of Software Testers in Test Log Management\nSoftware tester\u2019s role is paramount as the test log is created, maintained and updated by them. Software tester has to be technically sound in order to understand the domain and product requirements. Below are some of the salient activities performed by software tester:\n- Testers are responsible for creating and updating the test log which includes all the principal details such as test cases, test results, test artifacts included, bugs identified with status, priority and severity.\n- Testers need to review the test logs to make sure that all the information has been captured correctly and is complete.\n- Testers need to analyse test logs to recognize trends, patterns and issues that may impact the quality of the software being tested.\n- Testers need to work in collaboration and closely with different cross functional teams to guarantee that the testing process is smooth, effective and helpful for the entire team.\n- Testers are liable to maintain and improve the testing process on a timely basis. They need to review the test logs, improve and implement changes to make the testing process effective.\nWhat is log storage in testing?\nStoring and managing the test logs during test execution is termed as Log Storage in Software Testing. As creating a test log is important, storing and accessing it is equally important to allow the testers to access, analyse and modify at any given time. Following are some of the significances of storing test logs:\n- It is easy to identify the root cause of the problem by referring to the stored test logs for any software failure as test logs contain all the wealth of information pertaining to the test execution.\n- Using stored test logs, testers can create a report and trend of bugs found for the trail of test execution thereby showcasing the health of the software in a graphical manner.\n- Stored test logs can be accessed by any team member and it can serve as evidence that the test execution was executed properly and all the important details were captured.\nBrowserStack is a cloud-based platform that allows testing of mobile applications and websites across 3000+ on-demand browsers, operating systems and real devices. Along with testing, it also leverages the benefit of viewing and analysing test logs!\nTo view the test logs in BrowserStack, you can navigate to the Automate or Live section of the platform and click on the specific test run.\nFollowing image displays the test run details of Samsung Galaxy S22, 12.0 which was run on BrowserStack\u2019s Automate platform. The details of the test execution can be viewed after clicking on the particular test case.\nTest log generated by BrowserStack contains detailed information about the test execution such as test case name and description, environment details, steps executed during the run, test results, screenshots and videos of the test run, console and network logs.\nText logs can be viewed by clicking on the Text Logs tab shown in image below.\nSimilarly, Appium logs can be viewed by clicking on Other Logs >> Appium tab\nBrowserStack also provides integrations with popular test management tools like JIRA, Trello, and TestRail, allowing you to automatically capture and store test logs in your preferred system.",
      "domain": "browserstack.com",
      "retrieved_at": "2025-12-15 07:35:53.611878",
      "query_used": "test logging"
    },
    {
      "url": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
      "title": "How to test logging in Node.js: practical TDD tips and tricks",
      "content": "You know what gets swept under the rug and forgotten during TDD a lot? Logs.\nAfter many years of test-driven development (and/or pretending to do test-driven development), testing is still more of an art than a science to me in many cases off the beaten path. Especially determining exactly what to test, and how much testing is \u201cgood enough.\u201d\nI would argue that, unless the worried face on the man in this oil panting I commissioned to illustrate my point1 sparks joy for you, zero tests for logging is not \u201cgood enough.\u201d\nHowever, don\u2019t feel bad if you\u2019ve skipped testing your logging; I haven\u2019t done a survey or anything, but you\u2019re certainly not alone \u2014 after all it\u2019s not a \u201cuser-facing feature\u201d\u2026 I mean, unless you care about the experience of the people maintaining the software, which is to say your own experience, so\u2026 maybe\u2026. Anyway, regardless of that, I found it to be a surprisingly non-trivial task in a recent feature I implemented because of our logging library, winston. So I\u2019m here to help.\nNow don\u2019t get me wrong, winston is a great logger for Node.js apps. It\u2019s highly configurable, is feature-rich and makes it easy to configure a default logging format while adding extra metadata depending on the context of the module doing the logging, using \u201cchild\u201d loggers. However, it can be tedious and/or challenging to test file output in JavaScript, particularly a winston logger where the module under test uses logger.child\nto add metadata that you want to test \u2014 like this one:\nconst defaultLogger = require('../lib/logger') // winston.createLogger(...)\nconst logger = defaultLogger.child({ label: 'job:processTheThings' })\nexport default function processTheThings() {\n// some lets and stuff\nlogger.info('starting process')\n// some important stuff\nlogger.info('finished process', {success: true})\n}\nHow do we test this? Ideally, in my test, I want to be able to just say something like this to test the output, including any expected metadata (using testdouble.js, of course, but I\u2019ll add the jest version in comments for those who prefer it):\nconst logger = td.replace('../lib/logger') // or jest.createMockFromModule(...)\nconst subject = require('./processTheThings.js')\nsubject()\ntd.verify(logger.info('finished process',{ // or expect(logger.info).toHaveBeenCalledWith(...)\nlabel: 'job:processTheThings',\nsuccess: true\n}))\nThe problem: Child\nThis test would fail though, because my module doesn\u2019t call logger.info\n, it calls logger.child\n. I could mock that as well, but I don\u2019t want my test to be coupled to the module\u2019s implementation by expecting logger.child\nto be called specifically (that would make for a brittle test).\n\u201cWhy not just test the output?\u201d you may ask (at least that\u2019s what I asked myself). But this logger happens to be writing to a file, so I can\u2019t just mock global.console\nand see what it says, and have you ever tried mocking/testing file output in Node.js without your test becoming hopelessly complex or coupled to the implementation? (See oil painting above.)\nWhat I really want is a mockLogger\nthat I can spy on without worrying about how many times child\nis called, if at all. I want to write the test in a family-tree-agnostic way, so to speak (i.e. just verify the job is getting done, regardless of how many children are involved, if any.2)\nSolution: Make one! (a mockLogger\n)\nSince our mocking libraries already do a pretty good job of creating mocks that look like real things (i.e. have mocked versions of all the methods and properties of the real thing), let\u2019s just start with that:\nconst mockLogger = td.replace('../lib/logger')\n// or ... = jest.createMockFromModule('../lib/logger')\nThe only catch is, we need to change the behavior of the child\nmethod so that it propagates messages sent to any child logger up to the parent for testing. We can address that with something like this:\n// Takes a mocked logger, and replaces the child method to simplify testing\nfunction mockChildLoggers(logger) {\nlogger.child = childArgs => buildChildLogger(logger, childArgs)\nreturn logger\n}\nconst mockLogger = mockChildLoggers(td.replace('../lib/logger'))\nBut what does buildChildLogger\nlook like? Well, ideally I want something that will just call .info\non the parent logger, with the extra metadata attached. That will make it very easy to test the output, using only the original mocked logger. But we need to do this for every log level, not just .info\n\u2014 and oooh winston. Good old winston lets you create as many custom log levels as you want! So rather than hard code them (which would couple our mock module to our winston config) let\u2019s use winston\u2019s .levels\nproperty to get a list of methods to replace. (You might think this wouldn\u2019t work seeing as how it\u2019s a mocked instance, but td.replace()\nand jest.createMockFromModule\nboth copy primitive property values to the mocked instance, so .levels\nwill return whatever levels your winston config defines)\n// mockChildLoggers.js\nexport default function mockChildLoggers(logger) {\nlogger.child = childArgs => buildChildLogger(logger, childArgs)\nreturn logger\n}\nfunction buildChildLogger(logger = {}, childArgs){\nconst childLogger = {}\n// For each logging method:\nfor (const logLevel in logger.levels) {\nchildLogger[logLevel] = (message, args) => {\n// Just call the same method on the parent, adding the extra metadata from childArgs\nlogger[logLevel](message, {...childArgs, ...args})\n}\n}\n// And just in case someone decides to call\nVoila! This creates a mock of a winston logger, with which you can verify any calls to info\nor error\n(or any other method) on the primary mocked logger, even if the module delegates to child loggers to do the actual logging.\nSo now, in my test, I can simply mock my logger import with this function and expect any required logging to be called directly on the mocked logger, with all the required metadata, regardless of whether it\u2019s passed in directly or through one or more child loggers. That is, I can define the requirement in the test, but the implementation can be changed without breaking the test. \ud83d\ude4c\nCheck out processTheThings.test.js\nnow:\nimport mockChildLoggers from '@/test/mockChildLoggers'\ndescribe('processTheThings', function() {\nlet logger, processTheThings\nbeforeEach(() => {\nlogger = mockChildLoggers(td.replace('../lib/logger'))\n// ...or mockChildLoggers(jest.createMockFromModule(...))\nprocessTheThings = require('./processTheThings').default\n})\nit('logs sucessful completion', async () => {\nprocessTheThings()\ntd.verify(logger.info('finished process',{\n// or expect(logger.info).toHaveBeenCalledWith(...)\nlabel: 'job:processTheThings',\nsuccess: true\n}))\n})\n})\nIs this good?\nPro: This test is not coupled to the implementation of the module (you can use .child\n, you can pass in metadata directly, you can do both)\nPro: This test is not coupled to the winston constructor in lib/logger\n(you can change the log transport and format, and use any method to initialize the logger, all without rewriting your tests, as long as the export is a winston logger)\nCon: This test IS coupled to the implementation of the logger itself (winston). If the module starts using something else for logging, we\u2019ll have to rewrite the test and/or mockLogger\n. But to be fair, the logging is what we\u2019re testing, and every test has to be coupled to something it can deterministically test\u2026 (unless your test framework is an LLM I suppose\u2026. \ud83e\udd14)\nThe takeaway\nLet\u2019s step back from this particular problem for a moment to see if there\u2019s anything more general we can learn.\nThe root cause of the problem here was a combination of 3 characteristics of the thing we wanted to test:\n- It was something we wanted to mock rather than test in an end-to-end style (in this case that\u2019s because testing the final output is a bit of a pain, but the reasons don\u2019t really matter).\n- It was non-trivial to mock and verify, due to its flexible API. The subject module had several options on how it might accomplish the same basic task using its collaborator module (i.e. the logger).\n- It used a collaborator that didn\u2019t expose a canonical testing interface.\nIf any of these three things weren\u2019t the case, we would have had a simple path for testing.\nThe most solvable of these is #3. We just needed to created a canonical testing interface, against which we could test requirements were met, regardless of which part of the flexible API was used to meet them. Given these circumstances, you might choose to write a fully independent mock of the module being tested. In our case that would look similar to buildChildLogger\n, but it would also be storing the log results somewhere, and have methods for checking/verifying which logs were written (it would almost look like a fake, since it\u2019s using similar logic to maintain an internal state parallel the real thing, but I would only call it a fake if the resulting state were used by the application, and in this case it\u2019s only used by the test framework).\nFortunately, testdouble.js and jest are both pretty good at mocking things, and verifying mocked things, so we didn\u2019t have to maintain any state! All we had to do was account for the flexibility of the API. No mocking library could know that .child(...).info(...)\ncounts as a call to .info(...)\n. So we still solved #3, just in a simpler way: in our case, the only flexible part of the API was .child()\n, so it was simple to delegate that to a similar call on the parent to create a canonical testing interface.\nHow reusable is this concept? Who knows\u2026 I think it depends on how easy it is to identify \u201ca canonical testing interface\u201d despite a module having a flexible API. I don\u2019t know of a name for the pattern that this style of mocking follows. It\u2019s probably just \u201cmocks.\u201d \ud83d\ude0f Mocking libraries have just gotten so good that I rarely end up having to do any manual mocking outside of complex cases. This was an interesting case for thinking about how to write a mock that can decouple tests from implementations, where the potential for coupling is the mocked module\u2019s flexible API.\n- 1 When I say \u201ccommissioned\u201d, technically I mean \u201cprompted\u201d\u2026 but listen, if my blogging gets popular enough, I would LOVE to pay an artist to oil paint some blog headers. \u21a9\ufe0e\n- 2 This \u2018regardless of how many children are involved\u2019 strategy is generally Not Recommended for use outside of the software industry. (Ahem, the chocolate industry) (Dinna fash: there is hope!) \u21a9\ufe0e",
      "domain": "testdouble.com",
      "retrieved_at": "2025-12-15 07:35:55.526398",
      "query_used": "test logging"
    }
  ],
  "knowledge_graph": {
    "entities": [
      "Test Log",
      "Software Testing Life Cycle (STLC)",
      "Test Case",
      "Test Environment",
      "Test Results",
      "Defects",
      "Root Cause Analysis (RCA)",
      "Test Logging",
      "Logging",
      "Application Logging",
      "Security Logging",
      "Test Artifacts",
      "Test Strategy",
      "Test Plan",
      "Test Management Systems",
      "Collaboration Tools",
      "Audit Trails",
      "Compliance Monitoring",
      "Data Subject Access",
      "Freedom of Information",
      "Test Logging in Node.js",
      "TDD",
      "Test-Driven Development",
      "Logging Library",
      "Winston",
      "Child Loggers",
      "Metadata",
      "Test Output",
      "Testdouble.js"
    ],
    "relationships": [
      {
        "source": "Test Log",
        "relation": "includes",
        "target": "Test Case",
        "citation": "https://www.lambdatest.com/learning-hub/test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Log",
        "relation": "includes",
        "target": "Test Environment",
        "citation": "https://www.browserstack.com/guide/what-is-test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Log",
        "relation": "includes",
        "target": "Test Results",
        "citation": "https://www.lambdatest.com/learning-hub/test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Log",
        "relation": "includes",
        "target": "Defects",
        "citation": "https://www.browserstack.com/guide/what-is-test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Log",
        "relation": "includes",
        "target": "Root Cause Analysis (RCA)",
        "citation": "https://www.lambdatest.com/learning-hub/test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Logging",
        "citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Application Logging",
        "citation": "https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Security Logging",
        "citation": "https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Test Artifacts",
        "citation": "https://www.lambdatest.com/learning-hub/test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Test Strategy",
        "citation": "https://www.lambdatest.com/learning-hub/test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Test Plan",
        "citation": "https://www.lambdatest.com/learning-hub/test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Test Management Systems",
        "citation": "https://www.browserstack.com/guide/what-is-test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Collaboration Tools",
        "citation": "https://www.browserstack.com/guide/what-is-test-log",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Audit Trails",
        "citation": "https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Compliance Monitoring",
        "citation": "https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Data Subject Access",
        "citation": "https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html",
        "credibility": 1.0
      },
      {
        "source": "Test Logging",
        "relation": "includes",
        "target": "Freedom of Information",
        "citation": "https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html",
        "credibility": 1.0
      },
      {
        "source": "Test Logging in Node.js",
        "relation": "includes",
        "target": "TDD",
        "citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "credibility": 1.0
      },
      {
        "source": "Test Logging in Node.js",
        "relation": "includes",
        "target": "Test-Driven Development",
        "citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "credibility": 1.0
      },
      {
        "source": "Test Logging in Node.js",
        "relation": "includes",
        "target": "Logging Library",
        "citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "credibility": 1.0
      },
      {
        "source": "Test Logging in Node.js",
        "relation": "includes",
        "target": "Winston",
        "citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "credibility": 1.0
      },
      {
        "source": "Test Logging in Node.js",
        "relation": "includes",
        "target": "Child Loggers",
        "citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "credibility": 1.0
      },
      {
        "source": "Test Logging in Node.js",
        "relation": "includes",
        "target": "Metadata",
        "citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "credibility": 1.0
      },
      {
        "source": "Test Logging in Node.js",
        "relation": "includes",
        "target": "Test Output",
        "citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "credibility": 1.0
      },
      {
        "source": "Test Logging in Node.js",
        "relation": "includes",
        "target": "Testdouble.js",
        "citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "credibility": 1.0
      }
    ],
    "conflicts": [
      {
        "point_of_contention": "The efficiency of logging",
        "side_a": "Claims logging is essential for software testing",
        "side_a_citation": "https://www.lambdatest.com/learning-hub/test-log",
        "side_b": "Claims logging is not necessary for software testing",
        "side_b_citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "side_a_credibility": 1.0,
        "side_b_credibility": 1.0
      },
      {
        "point_of_contention": "The importance of logging in Node.js",
        "side_a": "Claims logging is crucial for Node.js development",
        "side_a_citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "side_b": "Claims logging is not important for Node.js development",
        "side_b_citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "side_a_credibility": 1.0,
        "side_b_credibility": 1.0
      },
      {
        "point_of_contention": "The role of logging in software testing",
        "side_a": "Claims logging is essential for software testing",
        "side_a_citation": "https://www.lambdatest.com/learning-hub/test-log",
        "side_b": "Claims logging is not necessary for software testing",
        "side_b_citation": "https://testdouble.com/insights/testing-logging-in-node-js-with-tdd",
        "side_a_credibility": 1.0,
        "side_b_credibility": 1.0
      }
    ]
  },
  "adversarial_queries": [],
  "executed_queries": "{'test logging'}",
  "synthesis_report": null,
  "max_iterations": 3,
  "current_phase": "mapper",
  "status_message": "Mapper phase completed: extracted 29 entities, 25 relationships, 3 conflicts"
}